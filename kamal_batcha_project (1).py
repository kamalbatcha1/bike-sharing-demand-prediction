# -*- coding: utf-8 -*-
"""Kamal Batcha project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iD8k0he--uW97W_IzBikUh_S_8SzFf-0

# Predict Bike Sharing Demand with AutoGluon Template

## Project: Predict Bike Sharing Demand with AutoGluon
This notebook is a template with each step that you need to complete for the project.

Please fill in your code where there are explicit `?` markers in the notebook. You are welcome to add more cells and code as you see fit.

Once you have completed all the code implementations, please export your notebook as a HTML file so the reviews can view your code. Make sure you have all outputs correctly outputted.

`File-> Export Notebook As... -> Export Notebook as HTML`

There is a writeup to complete as well after all code implememtation is done. Please answer all questions and attach the necessary tables and charts. You can complete the writeup in either markdown or PDF.

Completing the code template and writeup template will cover all of the rubric points for this project.

The rubric contains "Stand Out Suggestions" for enhancing the project beyond the minimum requirements. The stand out suggestions are optional. If you decide to pursue the "stand out suggestions", you can include the code in this notebook and also discuss the results in the writeup file.

## Step 1: Create an account with Kaggle

### Create Kaggle Account and download API key
Below is example of steps to get the API username and key. Each student will have their own username and key.

1. Open account settings.
![kaggle1.png](attachment:kaggle1.png)
![kaggle2.png](attachment:kaggle2.png)
2. Scroll down to API and click Create New API Token.
![kaggle3.png](attachment:kaggle3.png)
![kaggle4.png](attachment:kaggle4.png)
3. Open up `kaggle.json` and use the username and key.
![kaggle5.png](attachment:kaggle5.png)

## Step 2: Download the Kaggle dataset using the kaggle python library

### Open up Sagemaker Studio and use starter template

1. Notebook should be using a `ml.t3.medium` instance (2 vCPU + 4 GiB)
2. Notebook should be using kernal: `Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)`

### Install packages
"""

!pip install -U pip
!pip install -U setuptools wheel
!pip install -U "mxnet<2.0.0" bokeh==2.0.1
!pip install autogluon --no-cache-dir
# Without --no-cache-dir, smaller aws instances may have trouble installing

"""### Setup Kaggle API Key"""

# create the .kaggle directory and an empty kaggle.json file
!mkdir -p /root/.kaggle
!touch /root/.kaggle/kaggle.json
!chmod 600 /root/.kaggle/kaggle.json

# Fill in your user name and key from creating the kaggle account and API token file

import json
kaggle_username = "kamalbatcha"
kaggle_key = "8ec2f4f98f174fc8ca6de85c07a56831"

# Save API token the kaggle.json file
with open("/root/.kaggle/kaggle.json", "w") as f:
    f.write(json.dumps({"username": "kamalbatcha", "key":"8ec2f4f98f174fc8ca6de85c07a56831"}))

"""### Download and explore dataset

### Go to the [bike sharing demand competition](https://www.kaggle.com/c/bike-sharing-demand) and agree to the terms
![kaggle6.png](attachment:kaggle6.png)
"""

# Download the dataset, it will be in a .zip file so you'll need to unzip it as well.
!kaggle competitions download -c bike-sharing-demand
# If you already downloaded it you can use the -o command to overwrite the file
!unzip -o bike-sharing-demand.zip

import pandas as pd
from autogluon.tabular import TabularPredictor

# Create the train dataset in pandas by reading the csv
# Set the parsing of the datetime column so you can use some of the `dt` features in pandas later
train = pd.read_csv("train.csv")
train.head()

# Simple output of the train dataset to view some of the min/max/varition of the dataset features.
train.describe()

# Create the test pandas dataframe in pandas by reading the csv, remember to parse the datetime!
test = pd.read_csv("test.csv")
test.head()

# Same thing as train and test dataset
submission = pd.read_csv("sampleSubmission.csv")
submission.head()

"""## Step 3: Train a model using AutoGluonâ€™s Tabular Prediction

Requirements:
* We are predicting `count`, so it is the label we are setting.
* Ignore `casual` and `registered` columns as they are also not present in the test dataset. 
* Use the `root_mean_squared_error` as the metric to use for evaluation.
* Set a time limit of 10 minutes (600 seconds).
* Use the preset `best_quality` to focus on creating the best model.
"""

drop_train_features=["casual","registered"]
train.drop(drop_train_features,axis=1,inplace=True)

train.info()

predictor=TabularPredictor(label="count",problem_type="regression",eval_metric="rmse").fit(train_data=train,time_limit=600,presets="best_quality")

"""### Review AutoGluon's training run with ranking of models that did the best."""

predictor.fit_summary()

"""### Create predictions from test dataset"""

predictor.leaderboard(silent=True).plot(kind="bar",x="model",y="score_val")

"""#### NOTE: Kaggle will reject the submission if we don't set everything to be > 0."""

predictions = predictor.predict(test)
predictions = { 'count': predictions}
predictions = pd.DataFrame(data=predictions)
predictions.head()

predictions[predictions['count']<0]=0

submission["count"] = predictions
submission.to_csv("submission1.csv", index=False)

"""### Set predictions to submission dataframe, save, and submit"""

!kaggle competitions submit -c bike-sharing-demand -f submission1.csv -m "first raw submission"

"""#### View submission via the command line or in the web browser under the competition's page - `My Submissions`"""

!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6

"""#### Initial score of `?`

## Step 4: Exploratory Data Analysis and Creating an additional feature
* Any additional feature will do, but a great suggestion would be to separate out the datetime into hour, day, or month parts.
"""

train1=pd.read_csv("train.csv")
test1= pd.read_csv("test.csv")

"""## Make category types for these so models know they are not just numbers
* AutoGluon originally sees these as ints, but in reality they are int representations of a category.
* Setting the dtype to category will classify these as categories in AutoGluon.
"""

train1.loc[:, 'datetime'] = pd.to_datetime(train1.loc[:, 'datetime'])
test1.loc[:, 'datetime'] = pd.to_datetime(test1.loc[:, 'datetime'])

train1['year'] =  pd.to_datetime(train1.loc[:, 'datetime']).dt.year
train1['month'] =  pd.to_datetime(train1.loc[:, 'datetime']).dt.month
train1['day'] =  pd.to_datetime(train1.loc[:, 'datetime']).dt.day
train1['hour']=  pd.to_datetime(train1.loc[:, 'datetime']).dt.hour
test1['year'] =  pd.to_datetime(test1.loc[:, 'datetime']).dt.year
test1['month'] =  pd.to_datetime(test1.loc[:, 'datetime']).dt.month
test1['day'] =  pd.to_datetime(test1.loc[:, 'datetime']).dt.day
test1['hour']=  pd.to_datetime(test1.loc[:, 'datetime']).dt.hour

train1["season"]=train1["season"].astype("category")
train1["weather"]=train1["weather"].astype("category")
test1["season"]=test1["season"].astype("category")
test1["weather"]=test1["weather"].astype("category")

drop_train_features=["casual","registered"]
train1.drop(drop_train_features,axis=1,inplace=True)

# View are new feature
train1.head()

train1.hist()

"""## Step 5: Rerun the model with the same settings as before, just with more features"""

predictor_new_features =TabularPredictor(label="count",problem_type="regression",eval_metric="rmse").fit(train_data=train1,time_limit=700,presets="best_quality")

predictor_new_features.fit_summary()

predictions= predictor_new_features.predict(test1)
predictions = { 'count': predictions}
predictions = pd.DataFrame(data=predictions)
predictions.head()

# Remember to set all negative values to zero
predictions[predictions['count']<0]=0

submission_new_features=pd.read_csv("sampleSubmission.csv")

# Same submitting predictions
submission_new_features["count"] = predictions
submission_new_features.to_csv("submission_new_features.csv", index=False)

!kaggle competitions submit -c bike-sharing-demand -f submission_new_features.csv -m "new features"

!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6

"""#### New Score of `?`

## Step 6: Hyper parameter optimization
* There are many options for hyper parameter optimization.
* Options are to change the AutoGluon higher level parameters or the individual model hyperparameters.
* The hyperparameters of the models themselves that are in AutoGluon. Those need the `hyperparameter` and `hyperparameter_tune_kwargs` arguments.
"""

train2=pd.read_csv("train.csv")
test2= pd.read_csv("test.csv")

train2.loc[:, 'datetime'] = pd.to_datetime(train2.loc[:, 'datetime'])
test2.loc[:, 'datetime'] = pd.to_datetime(test2.loc[:, 'datetime'])

train2['year'] =  pd.to_datetime(train2.loc[:, 'datetime']).dt.year
train2['month'] =  pd.to_datetime(train2.loc[:, 'datetime']).dt.month
train2['day'] =  pd.to_datetime(train2.loc[:, 'datetime']).dt.day
train2['hour']=  pd.to_datetime(train2.loc[:, 'datetime']).dt.hour
test2['year'] =  pd.to_datetime(test2.loc[:, 'datetime']).dt.year
test2['month'] =  pd.to_datetime(test2.loc[:, 'datetime']).dt.month
test2['day'] =  pd.to_datetime(test2.loc[:, 'datetime']).dt.day
test2['hour']=  pd.to_datetime(test2.loc[:, 'datetime']).dt.hour

train2["season"]=train2["season"].astype("category")
train2["weather"]=train2["weather"].astype("category")
test2["season"]=test2["season"].astype("category")
test2["weather"]=test2["weather"].astype("category")

drop_train_features=["casual","registered","datetime"]
train2.drop(drop_train_features,axis=1,inplace=True)

drop_test_features=["datetime"]
test2.drop(drop_test_features,axis=1,inplace=True)

import autogluon.core as ag
hp_tune=True
rf_option={
   
   'n_estimators': [100,125,150],
   'max_depth': [5, 7, 10],
   'min_samples_split': [2, 5, 10],
   'min_samples_leaf': [1, 2, 4],
   'max_features': ['auto', 'sqrt', 'log2']
}
gbm_options = {
    'num_boost_round':85,  
    'max_depth':5,
    
}
hyperparameters={
                 "GBM":gbm_options,"RF":rf_option,
}
num_trails=2
search_strategy = 'auto'

hyperparameter_tune_kwargs={
    "num_trails":num_trails,
    "searcher":search_strategy,
    "scheduler":"local",
}
predictor_new_hpo = TabularPredictor(label="count",problem_type="regression",eval_metric="rmse").fit(train_data=train2,time_limit=700,presets="best_quality",hyperparameters=hyperparameters,hyperparameter_tune_kwargs=hyperparameter_tune_kwargs)

predictor_new_hpo.fit_summary()

prediction_new_hpo = predictor_new_hpo.predict(test1)
prediction_new_hpo = { 'count': prediction_new_hpo}
prediction_new_hpo = pd.DataFrame(data=prediction_new_hpo)
prediction_new_hpo.head()

# Remember to set all negative values to zero

prediction_new_hpo[prediction_new_hpo['count']<0]=0

submission_new_hpo=pd.read_csv("sampleSubmission.csv")

# Same submitting predictions
submission_new_hpo["count"] = prediction_new_hpo
submission_new_hpo.to_csv("submission_new_hpo.csv", index=False)
submission_new_hpo

!kaggle competitions submit -c bike-sharing-demand -f submission_new_hpo.csv -m "new features with hyperparameters"

!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6

"""#### New Score of `?`

## Step 7: Write a Report
### Refer to the markdown file for the full report
### Creating plots and table for report
"""

# Taking the top model score from each training run and creating a line plot to show improvement
# You can create these in the notebook and save them to PNG or use some other tool (e.g. google sheets, excel)
fig = pd.DataFrame(
    {
        "model": ["initial", "add_features", "hpo"],
        "score": [1.79818, 0.64953 ,0.46423]
    }
).plot(x="model", y="score", figsize=(8, 6)).get_figure()
fig.savefig('model_train_score.png')

# Take the 3 kaggle scores and creating a line plot to show improvement
fig = pd.DataFrame(
    {
        "test_eval": ["initial", "add_features", "hpo"],
        "score": [1.79818,0.64953,0.46423]
    }
).plot(x="test_eval", y="score", figsize=(8, 6)).get_figure()
fig.savefig('model_test_score.png')

"""### Hyperparameter table"""

# The 3 hyperparameters we tuned with the kaggle score as the result
hp=pd.DataFrame({
    "model": ["initial", "add_features", "hpo"],
    "hpo1": ['default_vals', 'default_vals', 'n_estimators: 100,125,150'],
    "hpo2": ['default_vals', 'default_vals', 'num_boost_round:85'],
    "hpo3": ['default_vals', 'default_vals', 'max_depth:5, 7, 10']

})

hp.head()

